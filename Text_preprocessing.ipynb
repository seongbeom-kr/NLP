{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCZko8IWFycm",
        "outputId": "d2385882-0f38-4e94-db12-2afb6c0385c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 구글 드라이브 연결 - 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 텍스트 전처리의 개념\n",
        "- 자연어 처리(NLP) = ***전처리*** + 변환 + 분석수행\n",
        "- 전처리 = 일반적으로 공통적인 과정을 거침\n",
        "## 1.1 전처리가 필요한 이유\n",
        "  - 문자(text) = 문자열 표시 -> ***'기호의 순차순열'*** -> 문자 단위로 저장\n",
        "  - 하나의 문장을 이해할 때는 사용된 **단어들의 순차**로 이해\n",
        "  => 단어를 이해,순서에 따라 의미를 이해\n",
        "  - 컴퓨터에게 어떤 문장을 이해시키고 싶은 경우 -> 하나의 문자열로 이루어진 문장 혹은 문서를 단어 단위로 나눈 후에 이 단어들의 리스트로 변환\n",
        "\n",
        "## 1.2 전처리의 단계\n",
        "  - 전처리는 '주어진 텍스트에서 노이즈와 같이이 불필요한 제거, 문자을 표준 단위로 분리, 각 단어의 품사까지 파악하는 것'\n",
        "  - 단계:  \n",
        "    1. 정재(Cleaning) : 주어진 텍스트에서 불필요한 노이즈를 제거. 토큰화 이전, 이후에도 간간히 지속적으로 진행\n",
        "    2. 토큰화(Tokenization) : 주어진 텍스트를 원하는 단위(토큰)로 나누는 작업. 문장 토큰화, 단어 토큰화\n",
        "    3. 정규화 : 같은 의미를 가진 동일한 단어임에도 불구하고 다른 형태로 쓰여진 단어들을 통일시켜 표준단어를 만듦( go, goes -> go), 어간추출과 표제어 추출이 있음\n",
        "    4. 품사 태깅 : 단어를 문법적인 기능에 따라 분류, 문맥 파악을 필요로 함\n",
        "\n",
        "# 2. 토큰화\n",
        "\n"
      ],
      "metadata": {
        "id": "YHd0VLH1LIhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('webtext')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pocLIUQZGJgc",
        "outputId": "b3d9da3b-bf19-4216-acf7-24a2f36dd84a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 문장 토큰화\n",
        "- 여러 줄의 문장으로 이루어진 텍스트를 문장 단위로 토큰화"
      ],
      "metadata": {
        "id": "ZTxax7gceFDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"Hello everyone. It's good to see you. Let's start our Text mining class!\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "#. 주어진 텍스트를 문장 단위로 토큰화. 주로, . ! ? 사용\n",
        "print(sent_tokenize(para))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7ogR3swUsoW",
        "outputId": "6ea49ced-c731-456f-8451-be8477863bf7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyone.', \"It's good to see you.\", \"Let's start our Text mining class!\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어도 잘 되는 모습\n",
        "para_kor = '안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!'\n",
        "print(sent_tokenize(para_kor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaOfS9RqVGGD",
        "outputId": "b779a70d-f98a-4055-c898-a33a47d430a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 단어토큰화\n",
        "- 일반적인 토큰화, 단어 단위로 분리\n",
        "- word_tokenize와 WordPunctTokenizer는 다른 알고리즘\n",
        "- 한국어 단어 토큰화는 잘 안됨(최소 의미 단위가 형태소이기때문)"
      ],
      "metadata": {
        "id": "99f351waVV1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "  # 주어진 택스트를 word 단위로 tokenize함\n",
        "print(word_tokenize(para))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og63YZz1VI-I",
        "outputId": "74f13400-1466-453c-ca83-1da909c035ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'Text', 'mining', 'class', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "print(WordPunctTokenizer().tokenize(para))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxVEiQhQVI1B",
        "outputId": "18f6864b-bafa-4b3f-f5fb-b229616365a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'Text', 'mining', 'class', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(WordPunctTokenizer().tokenize(para_kor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL_UM4AOVVkq",
        "outputId": "4c4bf946-515f-4152-b088-f5671ad8bf06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 정규표현식을 이용한 토큰화\n",
        "- NLTK가 제공하는 함수를 사용하면 토큰화가 간편, 단 원하는대로 세밀하게 토큰화 어려움\n",
        "- 정규표현식을 사용\n",
        "- regex, regexp\n",
        "- 문자열에 대해 원하는 검색 패턴을 지정하는 방법"
      ],
      "metadata": {
        "id": "MFOw62MpUUu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [abc] : [ ]사이에 있는 문자abc와 매칭"
      ],
      "metadata": {
        "id": "2CN7ly_gW5vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "re.findall(\"[abc]\", \"How are you, boy?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2BH0yn5LB_1",
        "outputId": "e7ea3825-4d31-457c-de09-c5b40d554f82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'b']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [0123456789] == [0-9]\n",
        "- [a-zA-Z] == a~z, A~Z\n",
        "- [\\w] == 알파벳 + 숫자 + _, 나머진 공백처리\n",
        "- []+ : 1회이상 반복, []{a,b} : a이상 b이하 반복"
      ],
      "metadata": {
        "id": "p1ecsAQhXHRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[0123456789]\",\"3a7b5c9d\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bo3uu2FW3ny",
        "outputId": "ff959b48-83be-421e-ed28-6955e8251a99"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3', '7', '5', '9']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[\\w]\",\"3a 7b_' .^&5c9d\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGaWjlNdXtkb",
        "outputId": "38d201cd-46c0-4323-a5d7-e8b15ccc3926"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[_]+\",\"a_b, c__d,e___f\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnZ5hUtmX04s",
        "outputId": "e6b7c7ed-6e61-4f0f-ddcb-132bb3208218"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_', '__', '___']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[\\w]+\", \"how are you, boy?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHUDNPNKYDeB",
        "outputId": "27d30700-6b5b-4e0a-f394-17af844a456e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how', 'are', 'you', 'boy']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[o]{2,4}\",\"oh, hoow are yooooou, boooooooy?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tSfDIXnYJbp",
        "outputId": "18df68c7-7ceb-410b-c7d1-77a3d09feb9d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oo', 'oooo', 'oooo', 'ooo']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RegexpTokenizer"
      ],
      "metadata": {
        "id": "bUc8DwJxYVOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "\n",
        "print(tokenizer.tokenize(\"Sorry, i can't go there\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YJzZcwdYplz",
        "outputId": "e918b349-91f3-4d77-d967-31d96286ecb1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sorry', 'i', \"can't\", 'go', 'there']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "print(tokenizer.tokenize(\"Sorry, i can't go there\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBWOR1CAY3M-",
        "outputId": "da5f3a4d-45f0-4c1b-d54d-4c45b1b4aba6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sorry', 'i', 'can', 't', 'go', 'there']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\w']{3,}\")\n",
        "text1 =\"Sorry, i can't go there\"\n",
        "print(tokenizer.tokenize(text1.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ZecAdOY-ym",
        "outputId": "c793e596-8343-4c54-a74e-3a26f8226cb5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', \"can't\", 'there']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 노이즈와 불용어 제거\n",
        "- 정규표현식으로 특수문자같은 불필요한 문자나 노이즈 제거가능\n",
        "- 불용어 : 사용하는 단어, 분석에 별 의미 없는 단어\n",
        "- 빈도가 너무 적거나, 반대로 너무 많은 경우 ex) ㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n",
        "- 길이가 너무 짧은 단어들 - 정규표현식으로 제거\n",
        "- 불용어 사전 생성 & 사전 참조하여 불용어 삭제"
      ],
      "metadata": {
        "id": "7eJWzhLZZs3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "english_stops = set(stopwords.words('english')) # 반복되지 않도록 set\n",
        "\n",
        "text1 = \"Sorry, I couldn't go to movie yesterday\"\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "tokens = tokenizer.tokenize(text1.lower())\n",
        "\n",
        "result = [word for word in tokens if word not in english_stops]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_mJK709ejlG",
        "outputId": "77c7b5f1-9d4c-4062-ba45-bde4feea9746"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', 'go', 'movie', 'yesterday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_stops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIdHK6UyfdpU",
        "outputId": "422700c9-a724-4ce4-eacb-98f1ad5769ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"mustn't\", 'only', 'not', 'again', 'couldn', 'so', 'some', 'myself', 'at', 'd', 'its', 'was', 'above', 'm', \"you'll\", 'itself', \"she's\", 'how', 'few', \"won't\", 'she', 'weren', 'any', \"hasn't\", 'hadn', 'y', 'all', \"haven't\", \"should've\", 'ain', 'shan', 'down', 'isn', 'were', 'these', 'over', 'can', 'should', 'didn', 'off', 'then', 'a', 'more', 'our', 'below', 'but', 'wouldn', 'this', 'if', 'them', 'mustn', 'does', 'further', 'won', 'to', \"didn't\", 'aren', 'him', 'themselves', 'until', 'for', \"doesn't\", 'being', 'each', 's', 'where', 'we', 'wasn', \"weren't\", 'theirs', 're', 'by', 'he', \"it's\", 'through', \"isn't\", 'it', 'hasn', 'because', 'what', 'needn', 'on', \"couldn't\", 'or', 'than', 'me', 'which', 'don', 'under', 'there', \"hadn't\", 'hers', 'have', 'my', 've', 'had', \"mightn't\", 'those', \"shouldn't\", 'when', 'll', 'did', 'the', 'between', 'into', 'in', 'you', 'they', 'their', 'do', 'no', 'be', \"you've\", 'ourselves', 'and', 'himself', 'herself', 'yours', 'very', 'her', 'your', 'before', 'doesn', \"that'll\", 'who', \"you're\", 'been', 'while', 'out', \"wouldn't\", 'yourself', 'that', 'such', 'his', \"you'd\", 'i', 'has', 'against', 'mightn', 'about', 'yourselves', \"shan't\", \"needn't\", 'other', \"don't\", 'haven', 'with', 'is', 'most', 'here', 'from', 'same', 'during', 'just', 't', 'nor', 'as', 'once', 'whom', 'own', 'shouldn', 'now', 'after', \"aren't\", 'of', 'doing', 'up', 'are', 'why', 'o', 'am', 'an', 'ours', 'too', \"wasn't\", 'will', 'both', 'ma', 'having'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_stopword = ['i','go','to']\n",
        "\n",
        "result = [word for word in tokens if word not in my_stopword]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICgm4lLQfsvd",
        "outputId": "11920e2c-1693-4528-c453-664b433d04c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', \"couldn't\", 'movie', 'yesterday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 정규화"
      ],
      "metadata": {
        "id": "Hv71TVVlf6QI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정규화 :같은 의미를 가진 동일한 단어면서, 다른 형태로 쓰여진 단어들을 통일해 표준단어로 만드는 작업\n",
        "- ex) go, goes, 먹다, 먹었다 -> go, 먹다\n",
        "- 어간 추출과 표제어 추출로 나뉜다."
      ],
      "metadata": {
        "id": "3RV5VemrgBpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 어간추출\n",
        "- ***어간추출*** : \"**어형**이 변형된 단어로부터 접사 등을 제거하고 그 단어의 어간을 분리해 내는 작업\"\n",
        "- 단어 정리\n",
        "  - 어형 : 단어의 형태\n",
        "  - ***어간(stem)*** : 어형변화에서 변화하지 않는 부분\n",
        "  => 어간 : ***용언***의 바뀌지 않는 부분, 어미 : 바뀌는 부분\n",
        "  - 용언 : 서술하는 동사 & 형용사\n",
        "  - 어형변화 : 동일한 어간을 가지고, 동일한 품사를 유지하면서 ***그 어미를 여러가지로 변화시켜 그에 따라 문법적 기능도 변화하는 현상***\n",
        "  - 어형변화는 시간의 경과에 따라 바뀌는 통시적 어형변화, 문법적 현상에 따른 공시적 어형변화\n",
        "  \n",
        "- 영어는 우리나라말과 원리와 구조가 다르므로 어간추출이 다르다.\n",
        "- 영어의 경우 복수형을 단수형으로 바꾸는 작업\n",
        "- Porter & Lancaster stemmer가 존재\n",
        "\n",
        "### 스테머 알고리즘\n",
        "- 단어가 변형되는 규칙을 이용해 원형을 찾음 -> 항상 올바른 결과는 아님\n",
        "- 포터 스테머는 모든 단어가 같은 규칙에 따라 변환\n",
        "- Lancaster는 포터와 유사하지만 조금 다르다"
      ],
      "metadata": {
        "id": "XhyOkobg24zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "print(stemmer.stem(\"cook\"))\n",
        "print(stemmer.stem(\"cookery\"))\n",
        "print(stemmer.stem('cookbooks'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUtNlR6G5A-7",
        "outputId": "52c5f32d-84ba-406e-f8ad-78e47049f7e7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cook\n",
            "cookeri\n",
            "cookbook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para= \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
        "tokens = word_tokenize(para)\n",
        "print(tokens)\n",
        "result = [stemmer.stem(token) for token in tokens]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1WDPixL5U2l",
        "outputId": "5fab9bc6-fc9f-4717-db16-34ee26fe7c65"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
            "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lancaster\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('cooking'))\n",
        "print(stemmer.stem('cookery'))\n",
        "print(stemmer.stem('cookbooks'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqf-idZl52I-",
        "outputId": "e48e260d-9846-498d-9e25-f8395dac915e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cook\n",
            "cookery\n",
            "cookbook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 표제어 추출\n",
        "- 표제어 추출(Lemmatization)은 Lemma변환, lemma는 우리말로 '단어의 기본형'으로 번역, 즉 주어진 단어를 기본형으로 바꾼다는 뜻\n",
        "- 어간추출과 표제어 추출과의 차이\n",
        "  - 어간 추출 : 언어학적인 관점\n",
        "  - 표제어 추출 : 사전에 나오는 독립적 의미(의미적 관점)\n",
        "\n",
        "- WordNet을 사용\n",
        "- 품사를 잘 파악해야함"
      ],
      "metadata": {
        "id": "Y0-HQXKB6kgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize('cooking'))\n",
        "print(lemmatizer.lemmatize('cooking', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('cookery'))\n",
        "print(lemmatizer.lemmatize('cookbook'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUSHTCKn7bcj",
        "outputId": "0e3431a0-588f-4afd-fb23-a9ca3fb40d41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cooking\n",
            "cook\n",
            "cookery\n",
            "cookbook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A0gucvmF8Ku5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}